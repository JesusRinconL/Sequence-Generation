{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4f343e-4319-47dc-92b7-fab9ec359d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from scipy.ndimage import convolve\n",
    "import lpips\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.inception import inception_v3\n",
    "from torchvision import transforms\n",
    "from scipy.linalg import sqrtm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e2398b-1c74-4ebd-b46f-3738cd728f8f",
   "metadata": {},
   "source": [
    "## Calculate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95dd05-925a-4a0b-80a2-cc0f680ae1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = Image.open(\"C:\\\\Users\\\\Jesus\\\\Documents\\\\TFM\\\\PROYECTO4\\\\results\\\\dualseq-bt_rg-v1\\\\test_latest\\\\images\\\\PANC-1_14_real_B.png\").convert('L')\n",
    "generated_image = Image.open('C:\\\\Users\\\\Jesus\\\\Documents\\\\TFM\\\\PROYECTO4\\\\results\\\\dualseq-bt_rg-v1_recur-2\\\\dualseq-bt_rg-v1_sketch\\\\test_latest\\\\images\\\\step14_generated_fake_B.png').convert('L')\n",
    "\n",
    "original_array = np.array(original_image)\n",
    "generated_array = np.array(generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da592496-eb03-4563-98b8-c3035bd9574c",
   "metadata": {},
   "source": [
    "#### Run only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45509538-ef78-4cfb-a499-0a43431dd5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el modelo LPIPS\n",
    "loss_fn = lpips.LPIPS(net='vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f11f00-c1cc-4fdf-a46a-394d166245b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(original_image, generated_image):\n",
    "    # Convertir imágenes PIL a matrices NumPy\n",
    "    original_array = np.array(original_image)\n",
    "    generated_array = np.array(generated_image)\n",
    "    \n",
    "    ## Calcular PSNR\n",
    "    psnr_value = psnr(original_array, generated_array)\n",
    "    ## Calcular SSIM\n",
    "    ssim_value = ssim(original_array, generated_array, win_size=7)\n",
    "    ## Calcular MSE\n",
    "    mse = mean_squared_error(original_array.flatten(), generated_array.flatten())\n",
    "    ## Calcular MAE\n",
    "    mae = mean_absolute_error(original_array.flatten(), generated_array.flatten())\n",
    "    \n",
    "    ## Calcular LPIPS\n",
    "    # Transformaciones necesarias para las imágenes\n",
    "    transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()]) # Modificar en función tal tamaño empleado\n",
    "    # Aplicar transformaciones a las imágenes\n",
    "    original_img = transform(original_image)\n",
    "    generated_img = transform(generated_image)\n",
    "    # Preparar las imágenes para la entrada en la métrica LPIPS\n",
    "    original_img = original_img.unsqueeze(0)\n",
    "    generated_img = generated_img.unsqueeze(0)\n",
    "    \n",
    "    # Calcular el LPIPS entre las imágenes\n",
    "    lpips_value = loss_fn(original_img, generated_img).item()\n",
    "\n",
    "    ## Calcular correlación de Pearson\n",
    "    pearson_corr = np.corrcoef(original_array.flatten(), generated_array.flatten())[0, 1]\n",
    "\n",
    "    ## Calcular entropia cruzada\n",
    "    # Calcular histogramas normalizados de las imágenes\n",
    "    hist_original, _ = np.histogram(original_array.flatten(), bins=256, range=[0, 256], density=True)\n",
    "    hist_generated, _ = np.histogram(generated_array.flatten(), bins=256, range=[0, 256], density=True)\n",
    "    epsilon = 1e-10\n",
    "    hist_org = np.maximum(hist_original, epsilon)\n",
    "    hist_gen = np.maximum(hist_generated, epsilon)\n",
    "    cross_entropy = -np.sum(hist_org * np.log(hist_gen))\n",
    "    \n",
    "    ## Calcular la diferencia de histograma\n",
    "    # Calcular histogramas\n",
    "    hist_original_diff, _ = np.histogram(original_array.flatten(), bins=256, range=[0,256])\n",
    "    hist_generated_diff, _ = np.histogram(generated_array.flatten(), bins=256, range=[0,256])\n",
    "    # Normalizar histogramas\n",
    "    hist_original_norm = hist_original_diff / np.sum(hist_original_diff)\n",
    "    hist_generated_norm = hist_generated_diff / np.sum(hist_generated_diff)\n",
    "    # Calcular distancia de Bhattacharyya\n",
    "    hist_diff = np.sum(np.sqrt(hist_original_norm * hist_generated_norm))\n",
    "\n",
    "    ## Calcular los errores de gradiente\n",
    "    # Definir los kernels de Sobel para las derivadas X e Y\n",
    "    sobel_x = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]])\n",
    "    sobel_y = np.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]])\n",
    "    # Aplicar convolución para obtener gradientes\n",
    "    grad_x_original = convolve(original_image, sobel_x)\n",
    "    grad_y_original = convolve(original_image, sobel_y)\n",
    "    grad_x_generated = convolve(generated_image, sobel_x)\n",
    "    grad_y_generated = convolve(generated_image, sobel_y)\n",
    "    # Calcular diferencia de gradientes\n",
    "    grad_diff = np.mean(np.abs(grad_x_original - grad_x_generated) + np.abs(grad_y_original - grad_y_generated))\n",
    "\n",
    "    ## Calcular IoU\n",
    "    original_array = (original_array - original_array.min()) / (original_array.max() - original_array.min()) * 255\n",
    "    generated_array = (generated_array - generated_array.min()) / (generated_array.max() - generated_array.min()) * 255\n",
    "\n",
    "\n",
    "    original_binary = original_array > 128\n",
    "    generated_binary = generated_array > 128\n",
    "    \n",
    "    # Calcular la intersección y la unión\n",
    "    intersection = np.logical_and(original_binary, generated_binary).sum()\n",
    "    union = np.logical_or(original_binary, generated_binary).sum()\n",
    "    \n",
    "    # Evitar divisiones por cero\n",
    "    if union <= 0:\n",
    "        return psnr_value, mse, mae, ssim_value, lpips_value, pearson_corr, cross_entropy, hist_diff, grad_diff, 0.0\n",
    "    IoU = intersection / union\n",
    "\n",
    "    return psnr_value, mse, mae, ssim_value, lpips_value, pearson_corr, cross_entropy, hist_diff, grad_diff, IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75861995-1f37-4507-b396-f99173d47252",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_value, mse, mae, ssim_value, lpips, pearson_corr, cross_entropy, hist_diff, grad_diff, IoU = calculate_metrics(original_image, generated_image)\n",
    "\n",
    "print(\"PSNR:\", round(psnr_value, 2))\n",
    "print(\"Intersection Over Union:\", round(IoU, 2))\n",
    "print(\"MSE:\", round(mse, 2))\n",
    "print(\"MAE:\", round(mae, 2))\n",
    "print(\"SSIM:\", round(ssim_value, 2))\n",
    "print(\"LPIPS:\", round(lpips, 2))\n",
    "print(\"Correlacion de Pearson:\", round(pearson_corr, 2))\n",
    "print(\"Entropía cruzada:\", round(cross_entropy, 2))\n",
    "print(\"Diferencia de Histograma:\", round(hist_diff, 2))\n",
    "print(\"Errores de gradiente:\", round(grad_diff, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636bdd3a-1fc5-4062-986d-cb2f5d283bcf",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dec66db-54fd-493e-8f00-d69250f5d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_total = 0\n",
    "iou_total = 0\n",
    "mse_total = 0\n",
    "mae_total = 0\n",
    "ssim_total = 0\n",
    "lpips_total = 0\n",
    "pearson_corr_total = 0\n",
    "cross_entropy_total = 0\n",
    "hist_diff_total = 0\n",
    "grad_diff_total = 0\n",
    "contador = 0\n",
    "\n",
    "psnr_values = []\n",
    "iou_values = []\n",
    "mse_values = []\n",
    "mae_values = []\n",
    "ssim_values = []\n",
    "lpips_values = []\n",
    "pearson_corr_values = []\n",
    "cross_entropy_values = []\n",
    "hist_diff_values = []\n",
    "grad_diff_values = []\n",
    "\n",
    "for i in tqdm(range(0, 15)):\n",
    "    # Comprobar que existe la imagen\n",
    "    if not os.path.isfile(f\"C:\\\\Users\\\\Jesus\\\\Documents\\\\TFM\\\\PROYECTO4\\\\results\\\\dualseq-bt_rg-v1_recur-2\\\\dualseq-bt_rg-v1_sketch\\\\test_latest\\\\images\\\\step{i}_generated_fake_B.png\"):\n",
    "        continue\n",
    "        \n",
    "    # Cargar imágenes\n",
    "    original_image = Image.open(f\"C:\\\\Users\\\\Jesus\\\\Documents\\\\TFM\\\\PROYECTO4\\\\results\\\\dualseq-bt_rg-v1\\\\test_latest\\\\images\\\\PANC-1_{i+1}_real_B.png\").convert('L')\n",
    "    generated_image = Image.open(f'C:\\\\Users\\\\Jesus\\\\Documents\\\\TFM\\\\PROYECTO4\\\\results\\\\dualseq-bt_rg-v1_recur-2\\\\dualseq-bt_rg-v1_sketch\\\\test_latest\\\\images\\\\step{i}_generated_fake_B.png').convert('L')\n",
    "    \n",
    "    # Convertir imágenes PIL a matrices NumPy\n",
    "    original_array = np.array(original_image)\n",
    "    generated_array = np.array(generated_image)\n",
    "\n",
    "    # Ejecutar la función\n",
    "    psnr_value, mse, mae, ssim_value, lpips, pearson_corr, cross_entropy, hist_diff, grad_diff, iou = calculate_metrics(original_image, generated_image)\n",
    "\n",
    "    # Añade a la lista los valores de cada imagen\n",
    "    psnr_values.append(psnr_value)\n",
    "    iou_values.appdned(iou)\n",
    "    mse_values.append(mse)\n",
    "    mae_values.append(mae)\n",
    "    ssim_values.append(ssim_value)\n",
    "    lpips_values.append(lpips)\n",
    "    pearson_corr_values.append(pearson_corr)\n",
    "    cross_entropy_values.append(cross_entropy)\n",
    "    hist_diff_values.append(hist_diff)\n",
    "    grad_diff_values.append(grad_diff)\n",
    "\n",
    "    # Number of images\n",
    "    contador += 1\n",
    "    \n",
    "# Calcular las medias\n",
    "psnr_media = sum(psnr_values) / contador\n",
    "iou_media = sum(iou_values) / contador\n",
    "mse_media = sum(mse_values) / contador\n",
    "mae_media = sum(mae_values) / contador\n",
    "ssim_media = sum(ssim_values) / contador\n",
    "lpips_media = sum(lpips_values) / contador\n",
    "pearson_corr_media = sum(pearson_corr_values) / contador\n",
    "cross_entropy_media = sum(cross_entropy_values) / contador\n",
    "hist_diff_media = sum(hist_diff_values) / contador\n",
    "grad_diff_media = sum(grad_diff_values) / contador\n",
    "\n",
    "# Calcular la desviación típica\n",
    "psnr_std = np.std(psnr_values)\n",
    "iou_std = np.std(iou_values)\n",
    "mse_std = np.std(mse_values)\n",
    "mae_std = np.std(mae_values)\n",
    "ssim_std = np.std(ssim_values)\n",
    "lpips_std = np.std(lpips_values)\n",
    "pearson_corr_std = np.std(pearson_corr_values)\n",
    "cross_entropy_std = np.std(cross_entropy_values)\n",
    "hist_diff_std = np.std(hist_diff_values)\n",
    "grad_diff_std = np.std(grad_diff_values)\n",
    "\n",
    "# Métricas y valores\n",
    "metricas = ['PSNR', 'IoU', 'MSE', 'MAE', 'SSIM', 'LPIPS', 'Correlacion de Pearson', 'Entropia cruzada', 'Diferencia de histograma', 'Errores de gradiente']\n",
    "valores = [psnr_values, iou_values, mse_values, mae_values, ssim_values, lpips_values, \n",
    "           pearson_corr_values, cross_entropy_values, hist_diff_values, grad_diff_values]\n",
    "\n",
    "# Medias y desviaciones estándar\n",
    "medias = [psnr_media, iou_media, mse_media, mae_media, ssim_media, lpips_media, pearson_corr_media, cross_entropy_media, hist_diff_media, grad_diff_media]\n",
    "desviaciones = [psnr_std, iou_std, mse_std, mae_std, ssim_std, lpips_std, pearson_corr_std, cross_entropy_std, hist_diff_std, grad_diff_std]\n",
    "\n",
    "# Calcula el coeficiente de variación de cada métrica\n",
    "coeficientes_variacion = []\n",
    "for i, j in zip(medias, desviaciones):\n",
    "    cv = (j / i) * 100\n",
    "    coeficientes_variacion.append(cv)\n",
    "\n",
    "# Añadir a las lista que incluye todos los valores para su posterior representación\n",
    "valores.append(coeficientes_variacion)\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Las métricas obtenidas de \", contador, \"imágenes son:\")\n",
    "for i, j in zip(medias, metricas):\n",
    "    i = round(i, 3)\n",
    "    print(f'La media de {j} es: {i}')\n",
    "    \n",
    "print('-'*60)\n",
    "for i, j in zip(desviaciones, metricas):\n",
    "    i = round(i, 3)\n",
    "    print(f'La desviación típica de {j} es: {i}')\n",
    "\n",
    "print('-'*60)\n",
    "for i, j in zip(coeficientes_variacion, metricas):\n",
    "    i = round(i, 3)\n",
    "    print(f'El coeficiente de variación de {j} es: {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c96db0",
   "metadata": {},
   "source": [
    "## FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80406add",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(num_images, path_real, path_fake):\n",
    "    # Cargar modelo InceptionV3\n",
    "    model = inception_v3(pretrained=True, transform_input=False)\n",
    "    model.fc = torch.nn.Identity()  # Quitar la última capa para obtener características\n",
    "    model.eval()\n",
    "\n",
    "    # Transformaciones para InceptionV3\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalización Inception\n",
    "    ])\n",
    "\n",
    "    real_images = []\n",
    "    fake_images = []\n",
    "    # Cargar imágenes en bucle\n",
    "    contador = 0\n",
    "    for i in range(num_images):\n",
    "        try:\n",
    "            real_img = transform(Image.open(path_real.format(i)).convert('RGB'))\n",
    "            fake_img = transform(Image.open(path_fake.format(i)).convert('RGB'))\n",
    "            real_images.append(real_img)\n",
    "            fake_images.append(fake_img)\n",
    "            contador +=1\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "            \n",
    "\n",
    "    # Convertir listas en tensores\n",
    "    real_images = torch.stack(real_images)\n",
    "    fake_images = torch.stack(fake_images)\n",
    "\n",
    "    # Mover a GPU si está disponible\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    real_images, fake_images = real_images.to(device), fake_images.to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    # Obtener características\n",
    "    with torch.no_grad():\n",
    "        features_real = model(real_images).cpu().numpy()\n",
    "        features_fake = model(fake_images).cpu().numpy()\n",
    "\n",
    "    # Calcular estadísticas (media y covarianza)\n",
    "    mu_real, sigma_real = np.mean(features_real, axis=0), np.cov(features_real, rowvar=False)\n",
    "    mu_fake, sigma_fake = np.mean(features_fake, axis=0), np.cov(features_fake, rowvar=False)\n",
    "\n",
    "    # Calcular FID\n",
    "    diff = mu_real - mu_fake\n",
    "    cov_mean = sqrtm(sigma_real @ sigma_fake)\n",
    "\n",
    "    if np.iscomplexobj(cov_mean):\n",
    "        cov_mean = cov_mean.real\n",
    "\n",
    "    fid = np.sum(diff**2) + np.trace(sigma_real + sigma_fake - 2 * cov_mean)\n",
    "    return fid, contador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8e863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 1000  # Número de imágenes a evaluar\n",
    "base_path = \"C:\\\\Users\\\\Jesus\\\\Documents\\\\TFM\\\\PROYECTO4\\\\results\\\\dualseq-CSIC-top_left\\\\dualseq-bt_rg-v1_sketch\\\\test_latest\\\\images\\\\PANC-1_\"\n",
    "base_path2 = \"C:\\\\Users\\\\Jesus\\\\Documents\\\\TFM\\\\PROYECTO3\\\\csic\\\\top_left\\\\PANC-1_\"\n",
    "p1 = \"{}_real_B.jpg\"\n",
    "p2 = \"{}_fake_B.png\"\n",
    "p3 = \"{}_real_A.png\"\n",
    "p4 = \"{}_fake_A.png\"\n",
    "fid_value, images = calculate_fid(num_images, \n",
    "                          base_path2 + \"{}.jpg\", \n",
    "                          base_path + p2)\n",
    "\n",
    "print(f\"FID: {fid_value} for {images} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b189ad-6f19-41f2-a4f9-78723ba004fa",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0470db60-719c-43fa-a882-4e1254c0f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Métricas y datos para valores altos\n",
    "metricas_valores_altos = ['PSNR', 'MSE', 'MAE', 'Errores de gradiente']\n",
    "medias_valores_altos = [psnr_media, mse_media, mae_media, grad_diff_media]\n",
    "desviaciones_valores_altos = [psnr_std, mse_std, mae_std, grad_diff_std]\n",
    "\n",
    "# Métricas y datos para valores bajos\n",
    "metricas_valores_bajos = ['SSIM', 'LPIPS', 'Correlacion de Pearson', 'Entropia cruzada', 'Diferencia de histograma']\n",
    "medias_valores_bajos = [ssim_media, lpips_media, pearson_corr_media, cross_entropy_media, hist_diff_media]\n",
    "desviaciones_valores_bajos = [ssim_std, lpips_std, pearson_corr_std, cross_entropy_std, hist_diff_std]\n",
    "\n",
    "# Crear la figura con dos subgráficas\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 10))\n",
    "\n",
    "# Subgráfica para valores altos\n",
    "axs[0].errorbar(metricas_valores_altos, medias_valores_altos, yerr=desviaciones_valores_altos, fmt='o', capsize=5, label='Media ± Desviación')\n",
    "axs[0].set_xticklabels(metricas_valores_altos, rotation=45, ha='right')\n",
    "axs[0].set_ylabel('Valor')\n",
    "axs[0].set_title('Medias y Desviaciones Estándar de las Métricas con valores altos')\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Subgráfica para valores bajos\n",
    "axs[1].errorbar(metricas_valores_bajos, medias_valores_bajos, yerr=desviaciones_valores_bajos, fmt='o', capsize=5, label='Media ± Desviación')\n",
    "axs[1].set_xticklabels(metricas_valores_bajos, rotation=45, ha='right')\n",
    "axs[1].set_ylabel('Valor')\n",
    "axs[1].set_title('Medias y Desviaciones Estándar de las Métricas con valores bajos')\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "# Ajustar automáticamente el diseño para evitar superposiciones\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756d01d5-62a5-4bb7-a33e-7d98af0738a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de valores que quieres graficar\n",
    "valores_grandes = [psnr_values, mse_values, mae_values, grad_diff_values]\n",
    "valores_pequeños = [ssim_values, lpips_values, pearson_corr_values, cross_entropy_values, hist_diff_values]\n",
    "\n",
    "# Lista de etiquetas para el histograma\n",
    "etiquetas_pequeños = ['SSIM', 'LPIPS', 'Correlacion de Pearson', 'Entropia cruzada', 'Diferencia de histograma']\n",
    "etiquetas_grandes = ['PSNR', 'MSE', 'MAE', 'Errores de gradiente']\n",
    "\n",
    "# Crear figuras y ejes\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "# Diagrama de caja valores pequeños (Boxplot)\n",
    "axs[0].boxplot(valores_pequeños)\n",
    "axs[0].set_xticklabels(etiquetas_pequeños, rotation=45, ha='right', fontsize=18)\n",
    "axs[0].set_title('Boxplots de métricas con valores bajos', fontsize = 20)\n",
    "\n",
    "# Diagrama de caja valores grandes (Boxplot)\n",
    "axs[1].boxplot(valores_grandes)\n",
    "axs[1].set_xticklabels(etiquetas_grandes, rotation=45, ha='right', fontsize=18)\n",
    "axs[1].set_title('Boxplots de metricas con valores altos', fontsize = 20)\n",
    "\n",
    "# Ajustar diseño de las subfiguras\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrar los gráficos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9af0277-ec69-4f52-8181-d39e6a61507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear subgráficos de histogramas para cada métrica\n",
    "fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "# Aplanar la matriz de subgráficos para iterar fácilmente sobre ella\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, (valor, etiqueta) in enumerate(zip(valores, metricas)):\n",
    "    axs[i].hist(valor, bins=20, color='skyblue', edgecolor='black')\n",
    "    axs[i].set_xlabel('Magnitud', fontsize = 14)\n",
    "    axs[i].set_ylabel('Valor', fontsize = 14)\n",
    "    axs[i].set_title(f'Histograma de {etiqueta}', fontsize = 14)\n",
    "    axs[i].grid(True)\n",
    "\n",
    "# Ajustar automáticamente el diseño para evitar superposiciones\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
